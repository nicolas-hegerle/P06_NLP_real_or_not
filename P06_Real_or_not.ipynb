{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 06: TWITTER REAL OR NOT\n",
    "## Bloc nÂ°4 - Jedha - dsmft - Paris14\n",
    "### Nicolas Hegerle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports and function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, SimpleRNN\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from spacy import load\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as STOP_WORDS\n",
    "\n",
    "nlp = load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Function definition</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stop_words, nlp, link_start = 'http'):\n",
    "    \"\"\"\n",
    "    Pre-processes an input string for machine learning by removing any unwanted characters\n",
    "     \n",
    "    Arguments:\n",
    "    text: the string you want to transform\n",
    "    stop_words: the set of stop words you want to remove from the string\n",
    "    nlp: the spacy model you want to use for lemmatisation\n",
    "    link_start: string of characters representing the beginning of the links you want to remove Ex: to remove \"http://t.co/rOI2NSmEJJ\" pass \"http\"\n",
    "\n",
    "    Operations:\n",
    "    Removes all alpha numeric and double spaces from 'text'\n",
    "    Replaces all double spaces by single spaces in 'text'\n",
    "    Removes links starting with \"link_start\" before a white space or at the end of the line\n",
    "    Changes all text to lower and stips white spaces at the en or beginning of a line\n",
    "    Remove \"stop_words\" and lemmatozes \"text\" with \"nlp\"\n",
    "\n",
    "    Output:\n",
    "    The cleaned string\n",
    "\n",
    "    Usage example:\n",
    "\n",
    "    Given\n",
    "\n",
    "    text = \"Check these out: http://t.co/rOI2NSmEJJ http://t.co/LxTjc87KLS #nsfw\"\n",
    "    stop_words = spacy.lang.en.stop_words import STOP_WORDS\n",
    "    nlp = load(\"en_core_web_sm\") from spacy\n",
    "    link_start = 'http'\n",
    "\n",
    "    Returns\n",
    "    \n",
    "    \"check nsfw\"\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    text = ''.join(ch for ch in text if ch.isalnum() or ch == \" \") #remove all non alpha-numeric characters and double spaces\n",
    "    text = re.sub(r\" +\", \" \", text) #replace all double spaces with a single space\n",
    "    text = re.sub(r\"{}.*?(?=\\s)\".format(link_start), '', text) #remove all web links followed by a space\n",
    "    text = re.sub(r\"{}.+$\".format(link_start), '', text) #remove all web links at the end of a line\n",
    "    text = text.lower().strip() #change all string to lower and remove any spaces at the beginning or end of a string\n",
    "    text = \" \".join([token.lemma_ for token in nlp(text) if (token.lemma_ not in stop_words) & (token.text not in stop_words)]) #lemmatise and remove stop words\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: data loading and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Load the dataset and print basic stats</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train.csv\", encoding='utf8')\n",
    "df_test = pd.read_csv(\"test.csv\", encoding='utf8')\n",
    "\n",
    "print(\"Train data head:\")\n",
    "display(df_train.head())\n",
    "\n",
    "print(\"Test data head:\")\n",
    "display(df_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>\n",
    "- The Test datasets contains the data on which we will perform our predictions for the 'competition' after training our model <br>\n",
    "- The Train dataset contains the data that will be used to train, validate and test our model => three way split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape:  (7613, 5)\n",
      "\n",
      "Train dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n",
      "None\n",
      "\n",
      "Description of the train dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7613.000000</td>\n",
       "      <td>7552</td>\n",
       "      <td>5080</td>\n",
       "      <td>7613</td>\n",
       "      <td>7613.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>221</td>\n",
       "      <td>3341</td>\n",
       "      <td>7503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>fatalities</td>\n",
       "      <td>USA</td>\n",
       "      <td>11-Year-Old Boy Charged With Manslaughter of T...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "      <td>104</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5441.934848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.42966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3137.116090</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.49506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2734.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5408.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8146.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10873.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id     keyword location  \\\n",
       "count    7613.000000        7552     5080   \n",
       "unique           NaN         221     3341   \n",
       "top              NaN  fatalities      USA   \n",
       "freq             NaN          45      104   \n",
       "mean     5441.934848         NaN      NaN   \n",
       "std      3137.116090         NaN      NaN   \n",
       "min         1.000000         NaN      NaN   \n",
       "25%      2734.000000         NaN      NaN   \n",
       "50%      5408.000000         NaN      NaN   \n",
       "75%      8146.000000         NaN      NaN   \n",
       "max     10873.000000         NaN      NaN   \n",
       "\n",
       "                                                     text      target  \n",
       "count                                                7613  7613.00000  \n",
       "unique                                               7503         NaN  \n",
       "top     11-Year-Old Boy Charged With Manslaughter of T...         NaN  \n",
       "freq                                                   10         NaN  \n",
       "mean                                                  NaN     0.42966  \n",
       "std                                                   NaN     0.49506  \n",
       "min                                                   NaN     0.00000  \n",
       "25%                                                   NaN     0.00000  \n",
       "50%                                                   NaN     0.00000  \n",
       "75%                                                   NaN     1.00000  \n",
       "max                                                   NaN     1.00000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Train dataset\n",
    "print(\"Train dataset shape: \", df_train.shape)\n",
    "print(\"\\nTrain dataset info:\")\n",
    "print(df_train.info())\n",
    "print(\"\\nDescription of the train dataset:\")\n",
    "display(df_train.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7613 unique ids in the dataset.\n",
      "There are 3342 unique locations in the dataset.\n",
      "There are 222 unique keywrods for a total of 7552 in the dataset.\n"
     ]
    }
   ],
   "source": [
    "#Unique ids in the dataset\n",
    "print(\"There are {} unique ids in the dataset.\".format(len(df_train.id.unique())))\n",
    "print(\"There are {} unique locations in the dataset.\".format(len(df_train.location.unique())))\n",
    "print(\"There are {} unique keywrods for a total of 7552 in the dataset.\".format(len(df_train.keyword.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>\n",
    "- \"id\" feature will be discarded for training. <br>\n",
    "- \"keywords\" might bring valuable information but will have to figure out how to use that feature. <br>\n",
    "- Same for \"locations\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target class and frequency:\n",
      "0    0.57034\n",
      "1    0.42966\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Lets find out what the most frequent class of the target is\n",
    "print(\"Target class and frequency:\")\n",
    "print(df_train['target'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>\n",
    "- We are dealing with a binary classification problem where '0' = not an emergency and '1' = emergency. <br>\n",
    "- Both classes are balanced (57% '0' vs 43% '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Dummy classification model</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy score from the dummy model:  0.5703402075397347\n",
      "Baseline f1-score from the dummy model:  0.0\n"
     ]
    }
   ],
   "source": [
    "#Lets try a dummy model to get our baseline scores\n",
    "X = df_train['id']\n",
    "y = df_train['target']\n",
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(X, y)\n",
    "print('Baseline accuracy score from the dummy model: ', dummy_clf.score(X, y))\n",
    "print('Baseline f1-score from the dummy model: ', f1_score(y, dummy_clf.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"3\"> <ins>Note:</ins> dummy model from Sklearn not necessary since a base model would get 57% accuracy by only predicting the most frequent outcome of the target which happens to be '0' i.e, not an emergency...but why not use it...<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Baseline classification model using only the text data</ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the target and the data used for training\n",
    "tweets = df_train[['text', 'target']]\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got send photo ruby alaska smoke wildfires pou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "      <td>rockyfire update california hwy 20 close direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "      <td>flood disaster heavy rain cause flash flooding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "      <td>m hill fire wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "      <td>s emergency evacuation happen building street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "      <td>m afraid tornado come area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "      <td>people die heat wave far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "      <td>haha south tampa getting flood hah wait second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>rain flood florida tampabay tampa 18 19 day ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "      <td>flood bago myanmar arrive bago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "      <td>damage school bus 80 multi car crash breaking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "      <td>s man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "      <td>love fruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "      <td>summer lovely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "      <td>car fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>goooooooaaaaaal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>this is ridiculous....</td>\n",
       "      <td>0</td>\n",
       "      <td>ridiculous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>London is cool ;)</td>\n",
       "      <td>0</td>\n",
       "      <td>london cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Love skiing</td>\n",
       "      <td>0</td>\n",
       "      <td>love skiing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What a wonderful day!</td>\n",
       "      <td>0</td>\n",
       "      <td>wonderful day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LOOOOOOL</td>\n",
       "      <td>0</td>\n",
       "      <td>looooool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  target  \\\n",
       "0   Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1              Forest fire near La Ronge Sask. Canada       1   \n",
       "2   All residents asked to 'shelter in place' are ...       1   \n",
       "3   13,000 people receive #wildfires evacuation or...       1   \n",
       "4   Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "5   #RockyFire Update => California Hwy. 20 closed...       1   \n",
       "6   #flood #disaster Heavy rain causes flash flood...       1   \n",
       "7   I'm on top of the hill and I can see a fire in...       1   \n",
       "8   There's an emergency evacuation happening now ...       1   \n",
       "9   I'm afraid that the tornado is coming to our a...       1   \n",
       "10        Three people died from the heat wave so far       1   \n",
       "11  Haha South Tampa is getting flooded hah- WAIT ...       1   \n",
       "12  #raining #flooding #Florida #TampaBay #Tampa 1...       1   \n",
       "13            #Flood in Bago Myanmar #We arrived Bago       1   \n",
       "14  Damage to school bus on 80 in multi car crash ...       1   \n",
       "15                                     What's up man?       0   \n",
       "16                                      I love fruits       0   \n",
       "17                                   Summer is lovely       0   \n",
       "18                                  My car is so fast       0   \n",
       "19                       What a goooooooaaaaaal!!!!!!       0   \n",
       "20                             this is ridiculous....       0   \n",
       "21                                  London is cool ;)       0   \n",
       "22                                        Love skiing       0   \n",
       "23                              What a wonderful day!       0   \n",
       "24                                           LOOOOOOL       0   \n",
       "\n",
       "                                       cleaned_tweets  \n",
       "0                deed reason earthquake allah forgive  \n",
       "1               forest fire near la ronge sask canada  \n",
       "2   resident ask shelter place notify officer evac...  \n",
       "3   13000 people receive wildfire evacuation order...  \n",
       "4   got send photo ruby alaska smoke wildfires pou...  \n",
       "5   rockyfire update california hwy 20 close direc...  \n",
       "6   flood disaster heavy rain cause flash flooding...  \n",
       "7                                    m hill fire wood  \n",
       "8       s emergency evacuation happen building street  \n",
       "9                          m afraid tornado come area  \n",
       "10                           people die heat wave far  \n",
       "11  haha south tampa getting flood hah wait second...  \n",
       "12  rain flood florida tampabay tampa 18 19 day ve...  \n",
       "13                     flood bago myanmar arrive bago  \n",
       "14      damage school bus 80 multi car crash breaking  \n",
       "15                                              s man  \n",
       "16                                         love fruit  \n",
       "17                                      summer lovely  \n",
       "18                                           car fast  \n",
       "19                                    goooooooaaaaaal  \n",
       "20                                         ridiculous  \n",
       "21                                        london cool  \n",
       "22                                        love skiing  \n",
       "23                                      wonderful day  \n",
       "24                                           looooool  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess the text data using the ```clean_text``` function\n",
    "tweets['cleaned_tweets'] = tweets['text'].apply(lambda x: clean_text(x, STOP_WORDS, nlp, \"http\"))\n",
    "display(tweets.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train dataset:  (5709, 3)\n",
      "Shape of val dataset:  (1237, 3)\n",
      "Shape of test dataset:  (667, 3)\n",
      "\n",
      "Target frequencies in train set: \n",
      "0    0.568401\n",
      "1    0.431599\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Target frequencies in val set: \n",
      "0    0.586904\n",
      "1    0.413096\n",
      "Name: target, dtype: float64\n",
      "\n",
      "Target frequencies in test set: \n",
      "0    0.556222\n",
      "1    0.443778\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into train, val and test keeping the dataset structure\n",
    "df_train, df_val = train_test_split(tweets, test_size = 0.25, random_state = 123)\n",
    "df_val, df_test = train_test_split(df_val, test_size = 0.35, random_state = 123)\n",
    "\n",
    "print(\"Shape of train dataset: \", df_train.shape)\n",
    "print(\"Shape of val dataset: \", df_val.shape)\n",
    "print(\"Shape of test dataset: \", df_test.shape)\n",
    "\n",
    "print(\"\\nTarget frequencies in train set: \")\n",
    "print(df_train['target'].value_counts(normalize = True))\n",
    "print(\"\\nTarget frequencies in val set: \")\n",
    "print(df_val['target'].value_counts(normalize = True))\n",
    "print(\"\\nTarget frequencies in test set: \")\n",
    "print(df_test['target'].value_counts(normalize = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the word tokenizer to transform our words into tokens keeping all the words\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens found:  13037\n",
      "Train set:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>encoded_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>Motorcyclist bicyclist injured in Denver colli...</td>\n",
       "      <td>1</td>\n",
       "      <td>motorcyclist bicyclist injure denver collision...</td>\n",
       "      <td>[1211, 1212, 158, 988, 340, 1380, 7, 2982]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>@ellenfromnowon 7-speed nexus shifter Ã¥Â£9! (Fo...</td>\n",
       "      <td>0</td>\n",
       "      <td>ellenfromnowon 7speed nexus shifter Ã¥9 communi...</td>\n",
       "      <td>[4594, 4595, 4596, 4597, 4598, 843, 2983, 2984]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5577</th>\n",
       "      <td>Landslide caused by severe rainstorm kills 3 i...</td>\n",
       "      <td>1</td>\n",
       "      <td>landslide cause severe rainstorm kill 3 italia...</td>\n",
       "      <td>[320, 55, 161, 514, 11, 51, 4599]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>@KimKardashian can you please sign and share t...</td>\n",
       "      <td>0</td>\n",
       "      <td>kimkardashian sign share petition save wild ho...</td>\n",
       "      <td>[4600, 169, 600, 913, 86, 233, 567, 2272]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5628</th>\n",
       "      <td>y'all read 12000 Nigerian refugees repatriated...</td>\n",
       "      <td>1</td>\n",
       "      <td>read 12000 nigerian refugee repatriate cameroon</td>\n",
       "      <td>[162, 785, 601, 186, 844, 845]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "1750  Motorcyclist bicyclist injured in Denver colli...       1   \n",
       "6995  @ellenfromnowon 7-speed nexus shifter Ã¥Â£9! (Fo...       0   \n",
       "5577  Landslide caused by severe rainstorm kills 3 i...       1   \n",
       "252   @KimKardashian can you please sign and share t...       0   \n",
       "5628  y'all read 12000 Nigerian refugees repatriated...       1   \n",
       "\n",
       "                                         cleaned_tweets  \\\n",
       "1750  motorcyclist bicyclist injure denver collision...   \n",
       "6995  ellenfromnowon 7speed nexus shifter Ã¥9 communi...   \n",
       "5577  landslide cause severe rainstorm kill 3 italia...   \n",
       "252   kimkardashian sign share petition save wild ho...   \n",
       "5628    read 12000 nigerian refugee repatriate cameroon   \n",
       "\n",
       "                                       encoded_tweets  \n",
       "1750       [1211, 1212, 158, 988, 340, 1380, 7, 2982]  \n",
       "6995  [4594, 4595, 4596, 4597, 4598, 843, 2983, 2984]  \n",
       "5577                [320, 55, 161, 514, 11, 51, 4599]  \n",
       "252         [4600, 169, 600, 913, 86, 233, 567, 2272]  \n",
       "5628                   [162, 785, 601, 186, 844, 845]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val set:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>encoded_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>@ChristophersZen @HunterLove1995 @tblack yeah ...</td>\n",
       "      <td>0</td>\n",
       "      <td>christopherszen hunterlove1995 tblack yeah man...</td>\n",
       "      <td>[551, 19, 206, 18, 636]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3265</th>\n",
       "      <td>Why are you engulfed by low self-image? Take t...</td>\n",
       "      <td>0</td>\n",
       "      <td>engulf low selfimage quiz</td>\n",
       "      <td>[346, 587, 1357, 840]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6512</th>\n",
       "      <td>We learn and grow and become stronger as we fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>learn grow strong face survive trial pass thom...</td>\n",
       "      <td>[380, 571, 916, 170, 79, 1605, 546]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5375</th>\n",
       "      <td>I added a video to a @YouTube playlist http://...</td>\n",
       "      <td>0</td>\n",
       "      <td>add video youtube playlist   panic disco miss ...</td>\n",
       "      <td>[591, 12, 64, 1035, 92, 1430, 126, 4533, 1706,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6354</th>\n",
       "      <td>@whvholst @leashless And this is a structural ...</td>\n",
       "      <td>0</td>\n",
       "      <td>whvholst leashless structural problem failure ...</td>\n",
       "      <td>[395, 763, 289, 3549, 759]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "1113  @ChristophersZen @HunterLove1995 @tblack yeah ...       0   \n",
       "3265  Why are you engulfed by low self-image? Take t...       0   \n",
       "6512  We learn and grow and become stronger as we fa...       0   \n",
       "5375  I added a video to a @YouTube playlist http://...       0   \n",
       "6354  @whvholst @leashless And this is a structural ...       0   \n",
       "\n",
       "                                         cleaned_tweets  \\\n",
       "1113  christopherszen hunterlove1995 tblack yeah man...   \n",
       "3265                          engulf low selfimage quiz   \n",
       "6512  learn grow strong face survive trial pass thom...   \n",
       "5375  add video youtube playlist   panic disco miss ...   \n",
       "6354  whvholst leashless structural problem failure ...   \n",
       "\n",
       "                                         encoded_tweets  \n",
       "1113                            [551, 19, 206, 18, 636]  \n",
       "3265                              [346, 587, 1357, 840]  \n",
       "6512                [380, 571, 916, 170, 79, 1605, 546]  \n",
       "5375  [591, 12, 64, 1035, 92, 1430, 126, 4533, 1706,...  \n",
       "6354                         [395, 763, 289, 3549, 759]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>encoded_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7225</th>\n",
       "      <td>Hello Twitter i need some book bloggers and in...</td>\n",
       "      <td>0</td>\n",
       "      <td>hello twitter need book blogger interview book...</td>\n",
       "      <td>[1181, 298, 57, 409, 1069, 409, 60, 2883]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>I liked a @YouTube video http://t.co/BM0QEC7Pj...</td>\n",
       "      <td>0</td>\n",
       "      <td>like youtube video   eminem feat nate dogg til...</td>\n",
       "      <td>[1, 64, 12, 1058, 693, 35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838</th>\n",
       "      <td>@JakeGint the mass murder got her hot and both...</td>\n",
       "      <td>1</td>\n",
       "      <td>jakegint mass murder hot bother heart traditio...</td>\n",
       "      <td>[113, 228, 103, 4376, 210]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6330</th>\n",
       "      <td>VIDEO: Virgin Galactic crash: Brakes blamed: I...</td>\n",
       "      <td>1</td>\n",
       "      <td>video virgin galactic crash brake blame invest...</td>\n",
       "      <td>[12, 822, 874, 15, 1437, 810, 344, 822, 874, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6067</th>\n",
       "      <td>150-Foot Sinkhole Opens In Lowndes County Resi...</td>\n",
       "      <td>1</td>\n",
       "      <td>150foot sinkhole open lowndes county residenti...</td>\n",
       "      <td>[5857, 270, 419, 2524, 254, 2525, 249]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "7225  Hello Twitter i need some book bloggers and in...       0   \n",
       "1611  I liked a @YouTube video http://t.co/BM0QEC7Pj...       0   \n",
       "4838  @JakeGint the mass murder got her hot and both...       1   \n",
       "6330  VIDEO: Virgin Galactic crash: Brakes blamed: I...       1   \n",
       "6067  150-Foot Sinkhole Opens In Lowndes County Resi...       1   \n",
       "\n",
       "                                         cleaned_tweets  \\\n",
       "7225  hello twitter need book blogger interview book...   \n",
       "1611  like youtube video   eminem feat nate dogg til...   \n",
       "4838  jakegint mass murder hot bother heart traditio...   \n",
       "6330  video virgin galactic crash brake blame invest...   \n",
       "6067  150foot sinkhole open lowndes county residenti...   \n",
       "\n",
       "                                         encoded_tweets  \n",
       "7225          [1181, 298, 57, 409, 1069, 409, 60, 2883]  \n",
       "1611                         [1, 64, 12, 1058, 693, 35]  \n",
       "4838                         [113, 228, 103, 4376, 210]  \n",
       "6330  [12, 822, 874, 15, 1437, 810, 344, 822, 874, 1...  \n",
       "6067             [5857, 270, 419, 2524, 254, 2525, 249]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fit the tokenizer on the train texts\n",
    "tokenizer.fit_on_texts(df_train.cleaned_tweets)\n",
    "print(\"Number of tokens found: \", len(tokenizer.word_index))\n",
    "\n",
    "# get the tokenized sequences for the train, val and test sets\n",
    "df_train['encoded_tweets'] = tokenizer.texts_to_sequences(df_train.cleaned_tweets)\n",
    "df_val['encoded_tweets'] = tokenizer.texts_to_sequences(df_val.cleaned_tweets)\n",
    "df_test['encoded_tweets'] = tokenizer.texts_to_sequences(df_test.cleaned_tweets)\n",
    "\n",
    "print(\"Train set:\")\n",
    "display(df_train.head())\n",
    "\n",
    "print(\"\\nVal set:\")\n",
    "display(df_val.head())\n",
    "\n",
    "print(\"\\nTest set:\")\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded train set shape:  (5709, 25)\n",
      "Padded val set shape:  (1237, 25)\n",
      "Padded test set shape:  (667, 25)\n"
     ]
    }
   ],
   "source": [
    "# padding sequences so they all have the same length\n",
    "\n",
    "# get the max length for padding\n",
    "dataframes = [df_train, df_val, df_test]\n",
    "max_pad = max([max(df['encoded_tweets'].apply(lambda x: len(x))) for df in dataframes])\n",
    "\n",
    "train_pad = tf.keras.preprocessing.sequence.pad_sequences(df_train.encoded_tweets, padding = 'post', maxlen=max_pad)\n",
    "print(\"Padded train set shape: \", train_pad.shape)\n",
    "\n",
    "val_pad = tf.keras.preprocessing.sequence.pad_sequences(df_val.encoded_tweets, padding = 'post', maxlen=max_pad)\n",
    "print(\"Padded val set shape: \", val_pad.shape)\n",
    "\n",
    "test_pad = tf.keras.preprocessing.sequence.pad_sequences(df_test.encoded_tweets, padding = 'post', maxlen=max_pad)\n",
    "print(\"Padded test set shape: \", test_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the tensorflow datasets using tf.data.Dataset.from_tensor_slices\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_pad, df_train.target))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_pad, df_val.target))\n",
    "\n",
    "# organize the datasets per batch\n",
    "train_ds = train_ds.shuffle(len(train_ds)).batch(300)\n",
    "val_ds = val_ds.shuffle(len(val_ds)).batch(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 25, 25)            325950    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                416       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 326,383\n",
      "Trainable params: 326,383\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the model\n",
    "embedding_dim = max_pad\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 # number of words in the vocabulary\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "                    Embedding(vocab_size, embedding_dim, name = \"embedding\", input_length=max_pad),\n",
    "                    GlobalAveragePooling1D(),\n",
    "                    Dense(16, activation='relu'),\n",
    "                    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 14:00:26.794894: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-29 14:00:26.794901: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-12-29 14:00:26.795082: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "tb_callback = TensorBoard(log_dir=\"logs2\")\n",
    "\n",
    "\n",
    "cp_path = \"save_model/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=cp_path,\n",
    "    monitor='metric.name',\n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "model.save_weights(cp_path.format(epoch=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=tf.keras.metrics.BinaryAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 14:00:26.976129: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-12-29 14:00:26.977141: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 5ms/step - loss: 0.6845 - binary_accuracy: 0.5684 - val_loss: 0.6751 - val_binary_accuracy: 0.5869\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6747 - binary_accuracy: 0.5684 - val_loss: 0.6663 - val_binary_accuracy: 0.5869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 14:00:27.022465: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-29 14:00:27.022474: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-12-29 14:00:27.024996: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-12-29 14:00:27.026314: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-12-29 14:00:27.029476: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs2/train/plugins/profile/2021_12_29_14_00_27\n",
      "\n",
      "2021-12-29 14:00:27.030530: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs2/train/plugins/profile/2021_12_29_14_00_27/Nicolass-MBP.trace.json.gz\n",
      "2021-12-29 14:00:27.031870: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs2/train/plugins/profile/2021_12_29_14_00_27\n",
      "\n",
      "2021-12-29 14:00:27.031995: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs2/train/plugins/profile/2021_12_29_14_00_27/Nicolass-MBP.memory_profile.json.gz\n",
      "2021-12-29 14:00:27.032680: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs2/train/plugins/profile/2021_12_29_14_00_27\n",
      "Dumped tool data for xplane.pb to logs2/train/plugins/profile/2021_12_29_14_00_27/Nicolass-MBP.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs2/train/plugins/profile/2021_12_29_14_00_27/Nicolass-MBP.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs2/train/plugins/profile/2021_12_29_14_00_27/Nicolass-MBP.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs2/train/plugins/profile/2021_12_29_14_00_27/Nicolass-MBP.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs2/train/plugins/profile/2021_12_29_14_00_27/Nicolass-MBP.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6655 - binary_accuracy: 0.5684 - val_loss: 0.6582 - val_binary_accuracy: 0.5869\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6522 - binary_accuracy: 0.5724 - val_loss: 0.6460 - val_binary_accuracy: 0.5982\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6323 - binary_accuracy: 0.6318 - val_loss: 0.6303 - val_binary_accuracy: 0.6443\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6053 - binary_accuracy: 0.6667 - val_loss: 0.6097 - val_binary_accuracy: 0.6766\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5715 - binary_accuracy: 0.7450 - val_loss: 0.5864 - val_binary_accuracy: 0.7098\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5320 - binary_accuracy: 0.7877 - val_loss: 0.5612 - val_binary_accuracy: 0.7559\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4887 - binary_accuracy: 0.8411 - val_loss: 0.5359 - val_binary_accuracy: 0.7599\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4442 - binary_accuracy: 0.8672 - val_loss: 0.5129 - val_binary_accuracy: 0.7793\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4031 - binary_accuracy: 0.8905 - val_loss: 0.4949 - val_binary_accuracy: 0.7825\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3649 - binary_accuracy: 0.8961 - val_loss: 0.4804 - val_binary_accuracy: 0.7922\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3319 - binary_accuracy: 0.9005 - val_loss: 0.4700 - val_binary_accuracy: 0.8044\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3019 - binary_accuracy: 0.9115 - val_loss: 0.4628 - val_binary_accuracy: 0.7979\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.2767 - binary_accuracy: 0.9156 - val_loss: 0.4578 - val_binary_accuracy: 0.8044\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - ETA: 0s - loss: 0.2617 - binary_accuracy: 0.930 - 0s 3ms/step - loss: 0.2546 - binary_accuracy: 0.9224 - val_loss: 0.4571 - val_binary_accuracy: 0.8060\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.2365 - binary_accuracy: 0.9291 - val_loss: 0.4549 - val_binary_accuracy: 0.8076\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.2188 - binary_accuracy: 0.9329 - val_loss: 0.4560 - val_binary_accuracy: 0.8068\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2033 - binary_accuracy: 0.9389 - val_loss: 0.4574 - val_binary_accuracy: 0.8076\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.1895 - binary_accuracy: 0.9413 - val_loss: 0.4605 - val_binary_accuracy: 0.8044\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1779 - binary_accuracy: 0.9478 - val_loss: 0.4631 - val_binary_accuracy: 0.8052\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1670 - binary_accuracy: 0.9503 - val_loss: 0.4667 - val_binary_accuracy: 0.8068\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1568 - binary_accuracy: 0.9529 - val_loss: 0.4708 - val_binary_accuracy: 0.8068\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1474 - binary_accuracy: 0.9553 - val_loss: 0.4761 - val_binary_accuracy: 0.8011\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1399 - binary_accuracy: 0.9583 - val_loss: 0.4830 - val_binary_accuracy: 0.8060\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1317 - binary_accuracy: 0.9594 - val_loss: 0.4865 - val_binary_accuracy: 0.7987\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.1251 - binary_accuracy: 0.9620 - val_loss: 0.4913 - val_binary_accuracy: 0.7971\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1194 - binary_accuracy: 0.9637 - val_loss: 0.4977 - val_binary_accuracy: 0.7947\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1127 - binary_accuracy: 0.9669 - val_loss: 0.5031 - val_binary_accuracy: 0.7930\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1076 - binary_accuracy: 0.9678 - val_loss: 0.5076 - val_binary_accuracy: 0.7963\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=30,\n",
    "    callbacks=[tb_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs --port=0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>\n",
    "- Val loss starts to increase around epoch 16. <br>\n",
    "- We can reload the model from the checkpoint at that state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding <keras.layers.embeddings.Embedding object at 0x17f989430>\n",
      "global_average_pooling1d <keras.layers.pooling.GlobalAveragePooling1D object at 0x17f989f10>\n",
      "dense <keras.layers.core.Dense object at 0x281566b20>\n",
      "dense_1 <keras.layers.core.Dense object at 0x281566e80>\n"
     ]
    }
   ],
   "source": [
    "# show the layers to check the weights\n",
    "for layer in model.layers:\n",
    "  print(layer.name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_1/kernel:0' shape=(16, 1) dtype=float32, numpy=\n",
      "array([[-0.87761235],\n",
      "       [-1.1077374 ],\n",
      "       [ 1.186593  ],\n",
      "       [-0.7988235 ],\n",
      "       [-0.7298084 ],\n",
      "       [ 1.1113573 ],\n",
      "       [-0.9738524 ],\n",
      "       [ 0.28683892],\n",
      "       [-0.55095893],\n",
      "       [-0.849572  ],\n",
      "       [-1.0404971 ],\n",
      "       [ 1.028631  ],\n",
      "       [-0.10541636],\n",
      "       [ 0.9323704 ],\n",
      "       [ 0.12954219],\n",
      "       [ 0.06211418]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.09340438], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "#Print out the weights of the last Dense layer\n",
    "print(model.layers[3].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 293us/step - loss: 0.5237 - binary_accuracy: 0.7856\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model after 30 epochs\n",
    "loss, acc = model.evaluate(test_pad, df_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x284024f10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the weights of the model as they were at epoch 16\n",
    "model.load_weights(\"save_model/cp-0016.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_1/kernel:0' shape=(16, 1) dtype=float32, numpy=\n",
      "array([[-0.6898739 ],\n",
      "       [-0.9051994 ],\n",
      "       [ 1.0047169 ],\n",
      "       [-0.6097721 ],\n",
      "       [-0.5354649 ],\n",
      "       [ 0.9107268 ],\n",
      "       [-0.79944617],\n",
      "       [ 0.28706953],\n",
      "       [-0.55095893],\n",
      "       [-0.6784977 ],\n",
      "       [-0.87214464],\n",
      "       [ 0.8183246 ],\n",
      "       [-0.10547903],\n",
      "       [ 0.7233409 ],\n",
      "       [ 0.12969962],\n",
      "       [ 0.06216064]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.06305792], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "#Print the weights of the model verify that they were indeed updated ans restored to their state at epoch 16\n",
    "print(model.layers[3].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 278us/step - loss: 0.4536 - binary_accuracy: 0.8051\n"
     ]
    }
   ],
   "source": [
    "# re-evaluate the model to check that the accuracy is better than before\n",
    "loss, acc = model.evaluate(test_pad, df_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for the trained model is:  0.7670250896057348\n",
      "Accuracy for the trained model is:  0.8050974512743628\n"
     ]
    }
   ],
   "source": [
    "#Compute the F1 score for that model at epoch 16 as it is the score used for the contest\n",
    "\n",
    "y_pred = np.where(model.predict(test_pad).flatten() > 0.5, 1, 0)\n",
    "y_true = df_test.target\n",
    "print(\"F1-score for the trained model is: \", f1_score(y_true, y_pred))\n",
    "print(\"Accuracy for the trained model is: \", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix on train set : \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the confusion matrices to better understanf the model\n",
    "print(\"Confusion matrix on train set : \")\n",
    "cm_train = confusion_matrix(y_true, y_pred)\n",
    "disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train)\n",
    "disp_train.plot()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Make predictions on test and submit to contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test data and preprocess them as for model training\n",
    "df_test_kag = pd.read_csv(\"test.csv\", encoding='utf8')\n",
    "df_test_kag = df_test_kag.loc[:, ['id', 'text']]\n",
    "df_test_kag['cleaned_tweets'] = df_test_kag['text'].apply(lambda x: clean_text(x, STOP_WORDS, nlp, \"http\"))\n",
    "df_test_kag['encoded_tweets'] = tokenizer.texts_to_sequences(df_test_kag.cleaned_tweets)\n",
    "\n",
    "# check that the max sequence length is shorter than 25\n",
    "max_seq_l = max(df_test_kag['encoded_tweets'].apply(lambda x : len(x)))\n",
    "if  max_seq_l < 25:\n",
    "    df_test_kag_pad = tf.keras.preprocessing.sequence.pad_sequences(df_test_kag.encoded_tweets, padding = 'post', maxlen=max_pad)\n",
    "else:\n",
    "    print(f\"Max sequence length is {max_seq_l} which is greater than {max_pad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First lines of the transformed test dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>encoded_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[204, 1424, 46, 15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "      <td>[151, 176, 866, 89, 322, 1029]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond geese flee street save</td>\n",
       "      <td>[107, 2, 420, 2880, 2075, 365, 86]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse light spokane wildfire</td>\n",
       "      <td>[368, 239, 71]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kill 28 china taiwan</td>\n",
       "      <td>[353, 530, 11, 8701, 440, 1080]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   0                 Just happened a terrible car crash   \n",
       "1   2  Heard about #earthquake is different cities, s...   \n",
       "2   3  there is a forest fire at spot pond, geese are...   \n",
       "3   9           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                 cleaned_tweets  \\\n",
       "0                     happen terrible car crash   \n",
       "1      hear earthquake different city stay safe   \n",
       "2  forest fire spot pond geese flee street save   \n",
       "3             apocalypse light spokane wildfire   \n",
       "4         typhoon soudelor kill 28 china taiwan   \n",
       "\n",
       "                       encoded_tweets  \n",
       "0                 [204, 1424, 46, 15]  \n",
       "1      [151, 176, 866, 89, 322, 1029]  \n",
       "2  [107, 2, 420, 2880, 2075, 365, 86]  \n",
       "3                      [368, 239, 71]  \n",
       "4     [353, 530, 11, 8701, 440, 1080]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded text sequenced for prediction:\n",
      "[[ 204 1424   46   15    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 151  176  866   89  322 1029    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 107    2  420 2880 2075  365   86    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 368  239   71    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 353  530   11 8701  440 1080    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# print a few lines to check\n",
    "print(\"First lines of the transformed test dataframe:\")\n",
    "display(df_test_kag.head())\n",
    "\n",
    "print(\"Padded text sequenced for prediction:\")\n",
    "print(df_test_kag_pad[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>encoded_tweets</th>\n",
       "      <th>model_preds</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[204, 1424, 46, 15]</td>\n",
       "      <td>0.630260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "      <td>[151, 176, 866, 89, 322, 1029]</td>\n",
       "      <td>0.552399</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond geese flee street save</td>\n",
       "      <td>[107, 2, 420, 2880, 2075, 365, 86]</td>\n",
       "      <td>0.816260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse light spokane wildfire</td>\n",
       "      <td>[368, 239, 71]</td>\n",
       "      <td>0.490171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kill 28 china taiwan</td>\n",
       "      <td>[353, 530, 11, 8701, 440, 1080]</td>\n",
       "      <td>0.932655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   0                 Just happened a terrible car crash   \n",
       "1   2  Heard about #earthquake is different cities, s...   \n",
       "2   3  there is a forest fire at spot pond, geese are...   \n",
       "3   9           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                 cleaned_tweets  \\\n",
       "0                     happen terrible car crash   \n",
       "1      hear earthquake different city stay safe   \n",
       "2  forest fire spot pond geese flee street save   \n",
       "3             apocalypse light spokane wildfire   \n",
       "4         typhoon soudelor kill 28 china taiwan   \n",
       "\n",
       "                       encoded_tweets  model_preds  target  \n",
       "0                 [204, 1424, 46, 15]     0.630260       1  \n",
       "1      [151, 176, 866, 89, 322, 1029]     0.552399       1  \n",
       "2  [107, 2, 420, 2880, 2075, 365, 86]     0.816260       1  \n",
       "3                      [368, 239, 71]     0.490171       0  \n",
       "4     [353, 530, 11, 8701, 440, 1080]     0.932655       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test_kag['model_preds'] = model.predict(df_test_kag_pad)\n",
    "df_test_kag['target'] = df_test_kag['model_preds'].apply(lambda x: 1 if x >= 0.5 else 0)\n",
    "display(df_test_kag.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the predictions and ids for submission\n",
    "predictions = df_test_kag[['id', 'target']]\n",
    "display(predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions to a csv file for submission\n",
    "predictions.to_csv('submission.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>\n",
    "\n",
    "## Score and next stpes\n",
    "- ranked 323 out of 888 participants with score of 0.80294   \n",
    "- improve text processing => twitter is a 'special' language   \n",
    "- improve neural network with specific text processing RNNs but risk of increasing overfit\n",
    "- add in more features for model training\n",
    "- Word2Vec instead of Embedding?\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc3767bdaf7d0bc88fb7dbb10edc027471f4bf53b32a5146e96204f2a81ba66c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('miniforge_venv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
